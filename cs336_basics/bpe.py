from collections import Counter
from concurrent.futures import ProcessPoolExecutor

from cs336_basics.utils import (
    find_chunk_boundaries,
    get_most_frequent_pair,
    get_pair_counts,
    pretokenize_chunk,
    split_chunks,
)

num_of_processes: int = 100


def initialize_vocab(special_tokens: list[str]) -> dict[int, bytes]:
    """Initialize the vocabulary for BPE tokenizer with single-byte tokens
    and special tokens.
    Args:
        special_tokens (list[str]): A list of string special tokens to be added to the tokenizer vocabulary.

    Returns:
        dict[int, bytes]: A dictionary mapping token IDs to their byte representations.
    """
    vocab = {i: bytes([i]) for i in range(256)}  # Initialize with single-byte tokens
    for special_token in special_tokens:  # Add special tokens to the vocabulary
        vocab[len(vocab)] = special_token.encode("utf-8")
    return vocab


def paralleize_pretokenization(
    chunks: list[str], num_of_processes: int, special_tokens: list[str]
) -> Counter:
    """Pretokenize the input chunks in parallel.

    Args:
        chunks (list[str]): List of byte chunks to be pretokenized.
        num_of_processes (int): Number of processes to use for parallelization.
        special_tokens (list[str]): List of special tokens to be preserved during pretokenization.

    Returns:
        Counter: A Counter object containing all pretokenized tokens.
    """
    with ProcessPoolExecutor(max_workers=num_of_processes) as executor:
        chunk_counters = list(
            executor.map(pretokenize_chunk, chunks, [special_tokens] * len(chunks))
        )
    # Combine all counters into a single Counter
    combined_counter = Counter()
    for counter in chunk_counters:
        combined_counter.update(counter)
    return combined_counter


def bpe_merge(
    counter: Counter, vocab: dict[int, bytes], merge_times: int
) -> tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:
    """Perform BPE merges on the given counter and vocabulary.
    Args:
        counter (Counter): A Counter object containing word frequencies, generated by pretokenization.
        vocab (dict[int, bytes]): The current vocabulary mapping token IDs to their byte representations.
        merge_times (int): The number of BPE merges to perform.

    Returns:
        tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:
            vocab:
                The trained tokenizer vocabulary, a mapping from int (token ID in the vocabulary)
                to bytes (token bytes)
            merges:
                BPE merges. Each list item is a tuple of bytes (<token1>, <token2>),
                representing that <token1> was merged with <token2>.
                Merges are ordered by order of creation.
    """
    pair_counts, word_splits, pairs_to_word = get_pair_counts(counter)
    merges = []

    for _ in range(merge_times):
        if not pair_counts:
            break

        most_frequent_pair = get_most_frequent_pair(pair_counts)
        if not most_frequent_pair or most_frequent_pair[0] is None:
            break

        token_A, token_B = most_frequent_pair
        new_token = token_A + token_B
        merges.append(most_frequent_pair)
        vocab[len(vocab)] = new_token

        words_to_update = list(pairs_to_word[most_frequent_pair])

        for word in words_to_update:
            tokens = word_splits[word]
            word_count = counter[word]

            # Decrement counts for all old pairs in this word
            for i in range(len(tokens) - 1):
                pair = (tokens[i], tokens[i + 1])
                if pair in pair_counts:
                    pair_counts[pair] -= word_count
                    if pair_counts[pair] <= 0:
                        del pair_counts[pair]
                        if pair in pairs_to_word:
                            pairs_to_word[pair].discard(word)
                            if not pairs_to_word[pair]:
                                del pairs_to_word[pair]

            # Create new token list for the word
            new_tokens = []
            j = 0
            while j < len(tokens):
                if j < len(tokens) - 1 and (tokens[j], tokens[j + 1]) == most_frequent_pair:
                    new_tokens.append(new_token)
                    j += 2
                else:
                    new_tokens.append(tokens[j])
                    j += 1
            word_splits[word] = new_tokens

            # Increment counts for all new pairs in this word
            for i in range(len(new_tokens) - 1):
                pair = (new_tokens[i], new_tokens[i + 1])
                pair_counts[pair] += word_count
                pairs_to_word[pair].add(word)

    return vocab, merges


def train_bpe(
    input_path: str, vocab_size: int, special_tokens: list[str]
) -> tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:
    """Given the path to an input corpus, run train a BPE tokenizer and
    output its vocabulary and merges.

    Args:
        input_path (str | os.PathLike): Path to BPE tokenizer training data.
        vocab_size (int): Total number of items in the tokenizer's vocabulary (including special tokens).
        special_tokens (list[str]): A list of string special tokens to be added to the tokenizer vocabulary.
            These strings will never be split into multiple tokens, and will always be
            kept as a single token. If these special tokens occur in the `input_path`,
            they are treated as any other string.

    Returns:
        tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:
            vocab:
                The trained tokenizer vocabulary, a mapping from int (token ID in the vocabulary)
                to bytes (token bytes)
            merges:
                BPE merges. Each list item is a tuple of bytes (<token1>, <token2>),
                representing that <token1> was merged with <token2>.
                Merges are ordered by order of creation.
    """
    vocab = initialize_vocab(special_tokens)

    # read input file
    with open(input_path, "rb") as f:
        # Split text into chunks for parallel processing of pretokenization
        chunk_boundaries = find_chunk_boundaries(
            f, num_of_processes, split_special_token=b"<|endoftext|>"
        )
        chunks = split_chunks(f, chunk_boundaries)
    pretokenize_counter = paralleize_pretokenization(
        chunks, num_of_processes, special_tokens
    )
    merge_times = vocab_size - len(vocab)  # Number of merges to perform
    vocab, merges = bpe_merge(pretokenize_counter, vocab, merge_times)
    return vocab, merges
