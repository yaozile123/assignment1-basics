from collections import Counter
from concurrent.futures import ProcessPoolExecutor

from cs336_basics.utils import (
    find_chunk_boundaries,
    get_most_frequent_pair,
    get_pair_counts,
    pretokenize_chunk,
    split_chunks,
    word_bytes_to_tokens,
    word_to_bytes,
)

num_of_processes: int = 100


def initialize_vocab(special_tokens: list[str]) -> dict[int, bytes]:
    """Initialize the vocabulary for BPE tokenizer with single-byte tokens
    and special tokens.
    Args:
        special_tokens (list[str]): A list of string special tokens to be added to the tokenizer vocabulary.

    Returns:
        dict[int, bytes]: A dictionary mapping token IDs to their byte representations.
    """
    vocab = {i: bytes(i) for i in range(256)}  # Initialize with single-byte tokens
    for special_token in special_tokens:  # Add special tokens to the vocabulary
        vocab[len(vocab)] = special_token.encode("utf-8")
    return vocab


def paralleize_pretokenization(
    chunks: list[str], num_of_processes: int, special_tokens: list[str]
) -> Counter:
    """Pretokenize the input chunks in parallel.

    Args:
        chunks (list[str]): List of byte chunks to be pretokenized.
        num_of_processes (int): Number of processes to use for parallelization.
        special_tokens (list[str]): List of special tokens to be preserved during pretokenization.

    Returns:
        Counter: A Counter object containing all pretokenized tokens.
    """
    with ProcessPoolExecutor(max_workers=num_of_processes) as executor:
        # chunk_counters = list(
        #     executor.map(lambda chunk: pretokenize_chunk(chunk, special_tokens), chunks)
        # )
        chunk_counters = list(
            executor.map(pretokenize_chunk, chunks, [special_tokens] * len(chunks))
        )
    # Combine all counters into a single Counter
    combined_counter = Counter()
    for counter in chunk_counters:
        combined_counter.update(counter)
    return combined_counter


def bpe_merge(
    counter: Counter, vocab: dict[int, bytes], merge_times: int
) -> tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:
    """Perform BPE merges on the given counter and vocabulary.
    Args:
        counter (Counter): A Counter object containing word frequencies, generated by pretokenization.
        vocab (dict[int, bytes]): The current vocabulary mapping token IDs to their byte representations.
        merge_times (int): The number of BPE merges to perform.

    Returns:
        tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:
            vocab:
                The trained tokenizer vocabulary, a mapping from int (token ID in the vocabulary)
                to bytes (token bytes)
            merges:
                BPE merges. Each list item is a tuple of bytes (<token1>, <token2>),
                representing that <token1> was merged with <token2>.
                Merges are ordered by order of creation.
    """
    pair_counts, word_to_pairs, pairs_to_word = get_pair_counts(counter)
    token_set = set(vocab.values())
    merges = []

    for _ in range(merge_times):
        # Find the most frequent pair
        if not pair_counts:
            break
        most_frequent_pair = get_most_frequent_pair(pair_counts)
        new_token = b"".join(most_frequent_pair)
        merges.append(most_frequent_pair)
        # Update vocabulary
        vocab[len(vocab)] = new_token
        token_set.add(new_token)

        # Update pair counts
        for affected_word in pairs_to_word[most_frequent_pair]:
            # Remove the old pairs
            for pair in word_to_pairs[affected_word]:
                if pair in pair_counts:
                    pair_counts[pair] -= word_to_pairs[affected_word][pair]
                    if pair_counts[pair] == 0:
                        del pair_counts[pair]
            # Add the new pair
            new_tokens = word_bytes_to_tokens(word_to_bytes(affected_word), token_set)
            for i in range(len(new_tokens) - 1):
                pair = (new_tokens[i], new_tokens[i + 1])
                pair_counts[pair] += counter[affected_word]
                word_to_pairs[affected_word][pair] += counter[affected_word]
                pairs_to_word[pair].add(affected_word)
        del pairs_to_word[most_frequent_pair]

    return vocab, merges


def train_bpe(
    input_path: str, vocab_size: int, special_tokens: list[str]
) -> tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:
    """Given the path to an input corpus, run train a BPE tokenizer and
    output its vocabulary and merges.

    Args:
        input_path (str | os.PathLike): Path to BPE tokenizer training data.
        vocab_size (int): Total number of items in the tokenizer's vocabulary (including special tokens).
        special_tokens (list[str]): A list of string special tokens to be added to the tokenizer vocabulary.
            These strings will never be split into multiple tokens, and will always be
            kept as a single token. If these special tokens occur in the `input_path`,
            they are treated as any other string.

    Returns:
        tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:
            vocab:
                The trained tokenizer vocabulary, a mapping from int (token ID in the vocabulary)
                to bytes (token bytes)
            merges:
                BPE merges. Each list item is a tuple of bytes (<token1>, <token2>),
                representing that <token1> was merged with <token2>.
                Merges are ordered by order of creation.
    """
    vocab = initialize_vocab(special_tokens)

    # read input file
    with open(input_path, "rb") as f:
        text = f.read()

    # Split text into chunks for parallel processing of pretokenization
    chunk_boundaries = find_chunk_boundaries(
        text, num_of_processes, split_special_token=b"<|endoftext|>"
    )
    chunks = split_chunks(text, chunk_boundaries)
    pretokenize_counter = paralleize_pretokenization(
        chunks, num_of_processes, special_tokens
    )
    merge_times = vocab_size - len(vocab)  # Number of merges to perform
    vocab, merges = bpe_merge(pretokenize_counter, vocab, merge_times)
    return vocab, merges
